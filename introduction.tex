\chapter{Introduction}
We report the exploration of improvements to a workflow management system that enables scientific methods to be encoded using data streaming. We report and evaluate a prototyping dynamic mapping of processes onto computational platforms.

The prevalence of workflow systems as a means of ongoing computational and data driven methods motivates this work. The nature of these systems and their data-streaming variants are introduced to set the context for the experiments. We then introduce \dpy as well as possible enhancements to it, and briefly describe our design to tackle them.

Science and researchers have gone through many eras and stages, and are facing a new paradigm today -- data-intensive science \cite{hey2009fourth}. Many current research campaigns involve producing large volumes of data and analysing them. As always, researchers need to develop methods themselves and then perform computation and analyse the results. Consequently researchers need flexible tools to help organise, implement and perform their methods. Here is how workflows and workflow management systems join the story: researchers can describe their methods by means of workflows and execute them in a workflow management system.

The scientific workflow (``workflow'' for short) is a technique used to organize and manage scientific computational jobs. By using scientific workflows, each computational job is decomposed into several (sub-)tasks which are connected by their dataflows (data dependencies). Hence, every workflow can be easily re-designed to suit specific needs by modifying its (sub-)tasks. Moreover, each (sub-)task can be seen as a module which can be developed independently, and they can be reused in different workflows so researchers can easily design new workflows by selecting and combining different modules.

The software system / framework used to execute scientific workflows is called a workflow management system (WMS). Researchers only need to provide the workflow and relevant input data, then the rest of the story will be automated by the WMS. Usually WMSs will parse the given workflow and map / assign tasks to some kinds of distributed computation platforms, such as clusters.

The \dpy system is a data-streaming workflow management system written in Python \cite{doi:10.1177/1094342016649766}. It does not introduce new execution platforms, but maps the workflow into some existing platforms such as MPI \cite{MPI_forum} or Apache Storm \cite{apache_storm}. As Python is widely available, the computers / servers on which the execution will happen don't need extra configurations. That said, as long as the libraries that tasks / PEs used are installed, \dpy can take control of the whole executing process and use the platform existing with no require of users' attention. If the tasks / PEs are written completely in Python, then no configuration is needed at all.

Currently, as far as we know, the workflow going to execute should be constructed solidly before its execution and is not subjected to change during its execution in most WMSs (which is all of the \emph{data-streaming} WMSs and most of the \emph{task-oriented} WMSs \footnote{The differenc between \emph{data-streaming} WMSs and \emph{task-oriented} WMSs will be detailed in the ``Background'' chapter.} with only a few exceptions, such as DAGMan for Condor \cite{couvares2007workflow}) and the deployment (\ie which node will execute which sub-task) happens at the beginning of the execution, which means the workflow as well as its deployment will be static during the execution. However, some computations can be represented more naturally in a dynamic way and some nodes (especially the ones near the end of the workflow) will actually be waiting for a long time before data coming in. As a consequence, it may save time or resources not to deploy all nodes in the beginning and may save nodes (therefore lower computational resources needed) if we can collect and re-allocate nodes that have finished producing outputs. To tackle this weakness, we extended \dpy to give it the ability to dynamically / incrementally deploy (sub-)tasks to computer nodes and a new semantics, \emph{dynamic expansion}, to workflows. In the meantime, our new system keeps backward compatibility so that existing workflows can still be correctly executed.

The ability to deploy (sub-)tasks to computer nodes only when needed is called \emph{incremental deployment} by us. We introduce a coordinator node that knows the global structure of the workflow and is in charge of node deployment, and degrade the rest of the nodes to executors / workers that only know themselves and their neighbours (\ie the nodes its input and output connections are connected with). Every time an executor needs new output targets, it requests that node and gets response from the coordinator. After all executors have finished their work (\ie no more inputs and outputs), they will all be shut down by the coordinator (and the coordinator also shuts itself down), so the whole execution is then finished. We use carefully designed signals / communications (which will be detailed in the \rcpt{Incremental Deployment} chapter) to ensure the execution and shut-down process's correct execution. This master-slave-like structure acts as a demonstration of possibilities for \dpy or other similar systems and is subject to change (\eg multiple coordinators \footnote{More details are presented in the \rcpt{Future Work} chapter.}) for future development.

The new semantics to grow the workflow dynamically according to certain rules is called \emph{dynamic expansion}. In order to keep backward compatibility, we add a special mark (implemented as a \lstinline|property| with default value) to the Processing Element (PE) indicating whether or not the task can be further expanded. In order to encode the expansion rules, we add special connections (called \emph{circuit connection}) and corresponding methods to define them. The details will be described in the ``Dynamic Expansion'' chapter.

Introducing these two new extensions enables us (or other researchers / developers) to make further optimisation and extensions. For example, in the original \dpy system, we can only collect performance data of past runs and use them to direct the deployment of future runs; but now, we can also collect the performance data during the incremental deployment (\ie on-the-fly) and use these data to aid the following deployment of the same execution.

The remaining part of this document will be organised as follows: first, we will present some terminology which may lead to confusion if not stated clearly; following that, we will present necessary background information; then we will detail the extensions we have done in separate chapters; after that, we will present the measurement and evaluation for them; finally, we will draw a conclusion.
