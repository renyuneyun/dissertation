\chapter{Incremental deployment}
\sout{The hypothesis and assumptions of dynamic deployment. \\
The design of the system in order to support dynamic deployment. \\
The details how we suit the design to the existing dispel4py framework.}

As said in the Background chapter, we expect future researchers would need to design workflows reading data from continuous infinite data sources. Therefore, the execution time of the workflow will also be infinite. Because of the variety of input data, we expect a dynamic way of optimising the target where a task should be deployed. To achieve this, we need first introduce a way to dynamically deploy tasks to computational nodes, and only then can make decisions on-the-fly.

Therefore, we introduce \emph{incremental deployment} to accomplish the job to deploy tasks dynamically. The following part of this chapter will first present how we bootstrap our design of the system; then give a high-order description of how we design the system to support \emph{incremental deployment}; later describe in depth how we organise and build each part; finally present how we change our modification to keep the compatibility of existing dispel4py mechanisms.

\section{Bootstraping the Design}
To deploy nodes incrementally, we will need a mechanism to guarantee:
\begin{enumerate}
	\item All PEs who needs a target can get a target.
	\item All PEs who are connected to the same PE (target) in the workflow will get the same target when we dynamically deploy PEs. That can be decomposed into two parts:
	\begin{enumerate}
		\item Two PEs requiring  the same target simultaneously will always get the same target.
		\item Two PEs requiring the same target at the same time will only trigger one deployment action.
	\end{enumerate}
	\item There is no lost of data during execution.
\end{enumerate}

Because of the nature of distributed computing, we couldn't find a low-cost way (\ie without many communications) to guarantees all these requirements. Therefore, we decide to introduce a coordinator whose main job it to handle the assignment / deployment and we will keep the transmission / communications minimal and keep the workload of the coordinator minimal to in case it becomes the bottleneck.

Basically, when a PE wants to send data through an output connection to a target, it will need to know the address of the target PE. If it doesn't know that address yet, it will need to request for the address. This could be done in a pair of communications - the first is from the PE to the coordinator (requiring the target), and the second is from the coordinator to the PE (informing the address of the target). If the target doesn't exist at the time of the request, the coordinator postpones the reply and tries to deploy the target first (and then sends the reply).

Naturally, when multiple PEs are requiring a same target at the same time, the coordinator could sequentialize these requests so only the first request actually triggers the deployment action while others simply wait until the deployment succeeds. There is also a possible scenario that one output connection connects to multiple targets. The above strategy can be easily extended to handle it: just check all possible targets (instead of assuming only one target there is) to see if another request is dealing with it. This can be done in either a sequential way or in parallel.

To reduce transmissions and save time, when a PE receives the target of an output connection, it caches the targets locally. When it wants to send data through this output connection again, it can directly use the cached information and there is no need to communicate to the coordinator again because, at our current stage, the deployed PEs won't move and all targets are deployed already when the coordinator sends the reply. Therefore, we can finalize the strategy to send data through a connection:

FLOW CHART HERE

In the above discussion, we assumes the strategy of deployment is known. However, we haven't really discussed it yet. Now is the time to design it.

To deploy a PE to a node, the coordinator sends a ``deploy'' communication with the PE that is going to be deployed to the node. We can, as provided by python, give all nodes the knowledge of all possible PEs. Therefore, the communication doesn't need to contain the whole object - only the name and / or necessary instructions to construct the PE can give the target node enough knowledge to construct the PE on its side. After constructing the target PE, the target node tells the coordinator that it is ready to receive data and then the coordinator can do other stuffs (\eg reply the address of this target when needed).

Apparently, a workflow will usually contain many PEs, so the coordinator will need to store the information that which PE is deployed to which node. When further requests come, the coordinator will first lookup to see if the PE is deployed or not.

Data are streamed directly from PEs (nodes) to PEs (nodes), and they won't pass through the coordinator.

The last question is how to determine when the execution of the workflow has finished (if the source is finite). We follow the existing design of the system to use a propagation fashion. By appending an ``end-of-stream'' marker to the end (\ie last unit) of each data steam, a PE can know how many previous nodes have finished producing outputs. Apparently, we will only need to give each PE the knowledge of the total number of previous nodes and then the PE can tell if all previous nodes have finished or not. If all previous nodes have finished producing outputs, the current node knows there won't be more inputs and therefore no more outputs will be produced by itself, so it can send (propagate) the ``end-of-stream'' marker to all its following PEs. Although it seems that we can do this on either PE level (when a PE has finished producing more data) or connection level (when no more data is going to be sent through a connection), in reality we prefer to do it on PE level because determining whether no more data is going to be sent through a connection requires the programmer of the PE to create extra logic to indicate this in a pipeline system.

\section{General design}	
As described above, we introduce a coordinator in addition to the PEs defined in the workflow, and degrades other nodes as executors (while still maintains the existing ``wrapper'' mechanism to PEs). The structure of our system is:

HERE SHOULD BE A FIGURE

The coordinator possesses the knowledge of the whole graph and current assignments, and handles deployment. Deployment happens when a node (which has been deployed previously) sends a ``require'' message along with the name of the output connection. When receiving this message, the coordinator first checks whether the target PE is already deployed or not: if it is already deployed, the coordinator simply replies it; if not, find nodes that are suitable to deploy, send deployment signal and reply with the assignment. Finally, when all nodes are finished (\ie no node is working and no deployment on-the-way), the coordinator also sends the ``finalize'' signal to all executors and then the whole execution of the workflow is shut down.
 
The main job of an executor is to execute one PE (in a wrapper) in the workflow. Apart from executing the PE (inside the wrapper), it also receives the deployment of itself (from coordinator) and receives shut-down (``finalize'') messages. All other actions are done in the wrapper.

A wrapper is the place that handles data receiving and sending of a PE. In addition to these original functions, we extend the wrapper to suit the need of incremental deployment - to request targets when an output is going to be sent through a new output connection, and wait for coordinator's reply. Specifically, an ``end-of-stream'' marker is sent after the last unit of data through all related output connections and the wrapper shuts itself down and returns control to the executor; when a wrapper receives enough (\ie the same as the sum of the number of nodes from each input connection) ``end-of-stream'' markers from its previous nodes, it then knows there is no more data and then propagates the ``end-of-stream'' marker through all its output connections the same way as previously described; in addition to propagation of the ``end-of-stream'' marker through the workflow graph, when each wrapper shuts itself down, it also sends a ``terminated'' signal to the coordinator so the coordinator knows this node is free (\ie back to the initial ``executor'' state and can then reassign it another PE if any.

Finally, there is an existing ``grouping'' mechanism which will create some duplicated PEs when deploying, and scatter data among them all so each of them could have fewer data to process and may reduce the total execution time. Our system introduces a local leader (called ``representative'') in each group who is in charge of the synchronization between groups. The detailed synchronisation strategy will be discussed below.

\section{Structure of Components}
After finishing the general design of the system, we will now suit the design to our actual platform. We will consider these points: efficiency and concurrency, synchronization, compatibility (of existing designs), and spawning (more node when needed).

\subsection{Efficiency and Concurrency}
We described a bit in the above  about the possibility of concurrency in the components. Making them run concurrently can usually bring better efficiency.

Because each PE is self-contained, we use multi-threading to achieve concurrency. It should be mentioned that each PE is designed to be single-threaded (at least, behaves like single-threaded).

The first is the coordinator. We can expect many communication to and from the coordinator run in parallel. Therefore, we will handle each request in a separate thread. Because multiple requests may act on a same PE, we will add a lock to each PE and acquire it when it is going to be act on (and release it when finished) so different requests of this same PE will be sequentialized.

Then is the executor. Because the function of executors is very simple and will only receive data, there is no need to make them run in multiple threads. Therefore, we design the executor to run in a single-threaded way.

Finally is the wrapper. Because a wrapper reads data from some input connections, processes the data previously read and writes processed output to some output connections, we run ``read'' in one thread, and run ``process'' and ``write'' together in another thread. If the order of data doesn't matter, we will run both ``process'' and ``write'' in several threads (in a thread pool). Because we need also request the address of targets, we need to design a suitable place to handle the communication to the coordinator. We decide to send the request at where is needed, and listen in another dedicated thread. The synchronous will be done by using condition variables - wait for the condition variable to become true after sending the request, and set it to true when receiving the reply.

\subsection{Synchronization}
One important aspect of distributed computing is to make sure the parallel execution won't break the needs of ordered messages \cite{â€¢}. Though not all messages need to be strictly ordered, some important messages should be kept ordered. Therefore, synchronization between nodes is very important.

In our system, there are several causalities which imply message ordering: receiving the target happens after target is deployed; ``end-of-stream'' marker sends and receives after all data units (which also implies shutting down only after receiving all data units). It becomes even more complicated when we try to handle the ``grouping'' mechanism: not all nodes inside a group will send data to some targets, so how to make sure shut-down propagation works correctly?

Luckily, MPI guarantees ordering of messages between each pair of nodes in one communicator \cite{MPI-3.0}, data won't be lost until it is received or cancelled and it provides several synchronization options for receiving and sending data. This slightly reduces our workload: the coordinator can always safely reply targets to the PE regardless of whether the target has finished initialization or not because the data won't be lost (so we don't need to do synchronization to make sure the target is ready); if we send the ``end-of-stream'' marker after all data units, the receiver side will not receive it before any data units.

However, we still need to carefully design the system especially when we try to keep the ``grouping'' mechanism. By introducing the local leader (\ie representative), we will use it to do synchronization of shut-down propagation from group to group, and all other nodes in that group (called ``brothers'' to the ``representative'') will not do synchronization across groups. Basically, we are aware that it is impossible for an unknown number of nodes (without doing explicit synchronization) to correctly propagate ``end-of-stream'' marker - because the number of sources is unknown to the receiver side. Therefore, we make the representative in charge of propagating ``end-of-stream'' marker. Roughly, the behaviour could be drawn like this:
\begin{enumerate}
	\item The ``end-of-stream'' marker will send to all groups and all nodes in each group.
	\item All nodes in the group who has finished producing more data will tell the representative that they are going to shut down and then they can shut down.
	\item Upon receiving a shut-down message from inside the group, the representative will check if that's the last node (including itself) who is going to shut down. If it is, then the representative propagates the ``end-of-stream'' marker to all following nodes and then shut itself down.
\end{enumerate}

This strategy can solve the problem of unknown number, but will introduce another problem: the order of messages is only guaranteed for each pair of nodes, but not across pairs of nodes. That means the ``end-of-stream'' marker sent by the representative (node A) may be received (by node R) prior to the last unit of data from one of the brothers (node B), where A and B are in one group.
 Therefore, we need a way to determine whether the receiver side has received all data from the previous group and then the representative can safely propagates the ``end-of-stream'' marker. With the combination of the 
\lstinline|MPI_Issend()| function and the \lstinline|MPI_Waitall()| function of MPI, we can make each brother wait until all the data sent from it have been received by the receiver side; then, the brother can tell the representative that it is going to shut itself down. Thus, when the representative knows all brothers are going to shut down, all data has already been received so propagating the ``end-of-stream'' marker at that time will not violate any constraints.

\subsection{Compatibility}
Compatibility here refers to both try to support all existing features and try to keep the existing structure of the design, because the current design of dispel4py is used in all parts (all mappings) so if we change one part of the core design, the rest of the code base usually should also be changed.

We have discussed some of the compatibility issues in the above part, such as how we keep the existing ``grouping'' feature while adding incremental deployment. In addition to that, we also slightly modified the design of PE to add an extra field (property) - FIFO. By defining the PE to be FIFO, the wrapper then knows it should not run multiple \lstinline|pe.process()| concurrently, but should execute them sequentially so the outputs are kept to be FIFO. Another important aspect that we haven't discussed yet is the \lstinline|Communication| class used in the dispel4py framework to decide which  target node should the current unit of data be streamed to. A \lstinline|Communication| is constructed with the addresses of target nodes and the connection type. When deciding the target of a unit of data, the name of the output connection is used. Therefore, we will construct the \lstinline|Communication| of an output connection right after the reply of target addresses.

\subsection{Spawning}
Traditionally, when running an MPI programme, the user needs to decide the number of nodes that is going to be used at the time of execution. However, this method has two drawbacks:
\begin{enumerate}
	\item Spawning nodes takes time, and the more nodes you spawn the more time it needs.
	\item Some PEs may terminate earlier than others so actually the maximum number of nodes running simultaneously may be smaller than the number of PEs in the workflow (so some nodes will be spawned but never used).
\end{enumerate}

Therefore, we expect to spawn nodes only when needed during running. MPI provides this mechanism by a function \lstinline|MPI_Comm_spawn()|. There is a configurable parameter which controls the number of nodes going to spawn at this time. The spawned nodes will be placed on a different \textit{intracommunicator} and connect with the existing nodes through an \textit{intercommunicator}. Because we want all executors to be able to communicate with each other and we may do the spawning for multiple times, we will also have to merge the \textit{intercommunicator} to a new \textit{intracommunicator} by using \lstinline|MPI_Intercomm_merge()|.

Then, we will need to design the strategy of communication under the scenario of spawning and merging. The strategy can be chosen between these two ends:
\begin{enumerate}
	\item Switch from old communicator to new communicator every time the new a communicator is created. 
	\item Use the old communicator whenever possible. 
\end{enumerate}

Our strategy choice is somewhat between these two: each communication channel always goes through the same \textit{communicator}; each every communication always uses the newest \textit{communicator}. 
