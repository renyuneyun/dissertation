\chapter{Background}
Because scientific workflows decompose computational jobs into smaller (loose-coupling) pieces, tasks, and use dataflow to connect them, various strategies can be chosen to accomplish the execution (and management) of workflows. Different choices of strategies lead to different behaviours of the workflow management systems (WMSs) and make them suitable for different scenarios. In this chapter, we introduce the growing use of scientific workflows, providing three examples in different disciplines, present the classification of workflow management systems (WMSs) and introduce some basic concepts of the WMS on which our work is based, \dpy. During the journey, we also gradually show why we favour data-streaming WMSs and why we choose \dpy.

\section{Workflow on Nowadays}
The grow and expansion of scientifc workflows went through many years. During the past 10 years, workflows have been adopted by growing number of communities and have influenced almost all researchers \cite{ATKINSON2017216}.
% Because of the abstract definition of workflows, researchers can always produce new methods and tools to refresh current implementations or methods. Malcolm \etal \cite{ATKINSON2017216} presented four benefits of it:
%\begin{enumerate}
%	\item The encoded meaning can sustain during the evolvement of technologies because they are largely independent of implementation.
%	\item The platforms can be mapped to are unlimited as of the semantics.
%	\item Components are easily reusable and re-designable.
%	\item Inter-discipline sharing is easy.
%\end{enumerate}

Many diciplines use workflows to perform their computational jobs. For example, in a recent paper about a new framework for gravitational wave detection \cite{gwave}, \citeauthor{gwave} use the workflows provided by PyCBC \footnote{https://ligo-cbc.github.io/} to perform the actual gravitational wave detection job. In seismology, seismic ambient noise cross-correlation, which is an important job for preprocessing / denoising raw data gathered, is often encoded as workflows and executed by a WMS (e.g. a hybrid system, Asterism \cite{Asterism}, uses seismic ambient noise cross-correlation as their demonstration). A recent workflow management system called DALiuGE \cite{wu2017daliuge} is demonstrated, in production, capable to process vary large quantities of data and is going to be used, as a prototype, in a consortium of Square Kilometre Array (SKA).

Although great progress and plenty of successful work (including those described above, and others \eg \cite{berriman2007generating} \cite{berriman2010application} in astrophysics, \cite{aiche2015workflows} in biochemistry) have been made, many areas are still under exploration in the field of workflow-related tools. For example, in the past time, most WMSs adopt the task-oriented style which may touch the bottleneck with the growing volume of scientific data whereas data-streaming style WMSs can significantly solve this problem \cite{doi:10.1177/1094342016649766}. Although we have the disk storage space, as Kryder’s law \citep{Kryders_law} pointed the storage density can double every 14 months, the IO speed doesn't increase that fast so caching the data of every stage onto disk will eventually become a huge bottleneck for task-oriented WMSs, unless new storage technologies emerge.

Another field is the workflow-sharing technologies. Different systems have different techniques to set up workflows or tasks, which may involve programming languages and paradigms. Therefore, although systems like myExperiment \cite{de2008design} and CrowdLabs \cite{Mates2011} exist, it is still hard to share tasks and workflows in a domain or implementation independent way. The only work we are aware of is the work by \citeauthor{GARIJO2017271} \cite{GARIJO2017271} which describes the semantics and a system to share workflows and tasks in the sense of the actual task they describe. 

\section{Task-oriented and Data-streaming}
Because the term workflow doesn't describe any details or standards, different systems usually use different methods to construct, manage and execute workflows. These systems are called workflow management systems (WMSs). Because each of them is composed of different trade-offs in their design, different WMSs have different characteristics and can not be roughly unified.

We see the way how WMSs schedule tasks and transport data as their main difference. Therefore we divide WMSs into these two types:
\begin{enumerate}
	\item Task oriented
	\item Data streaming
\end{enumerate}

\textbf{Task-oriented} WMSs usually decompose the workflow into each task, and execute them separately in different phases / stages. The data produced by each task will usually be stored to disk and feed into the following task (and then may be deleted). Systems like Kepler \cite{ludascher2006scientific}, KNIME \cite{Berthold:2009:KKI:1656274.1656280}, Galaxy \cite{blankenberg2010galaxy} and Pegasus \cite{deelman2015pegasus} are all developed as task-oriented workflow management systems and they are the majority of WMSs. The benefit of task-oriented WMSs is they have the entire control of the workflow's execution and can schedule the deployment according to needs (\ie the user can easily stop the execution at some point); they can also provide better fault-tolerance because data of each stage are cached on disk. However, this property is also its weakness: splitting tasks into stages will force a latter task to wait until all its previous tasks has finished, so the time needed will be longer. Moreover, it won't be able to support a continuous infinite data source because the stage used to execute that source will be infinite.

\textbf{Data-streaming} WMSs, on the other hand, stream small units of data directly from the prior tasks to the following tasks, often in a pipeline fashion. It is significantly different to task-oriented WMSs which move entire data between stages, and each unit of data can be arbitrarily complex (or arbitrarily simple such as a filename) in data-streaming systems \cite{doi:10.1177/1094342016649766}. Therefore, it will naturally support infinite data sources and there is no stages so no time will be wasted in waiting for the entire dataset is produced. As a result, the total execution time will be lower and users can get preliminary (partial) data when the first unit of outputs come out of the last task(s). On the contrary, fault tolerance of data-streaming WMSs will be weaker because there is no default mechanism caching intermediate data (so if a node fails, the whole workflow will need to restart). Even though, this weakness can be alleviated a bit by introducing intermediate tasks which passes the data while persisting them.

Although there are already several systems in the field, they don't completely satisfy the needs of current or near-future researchers \cite{•}. For example, with the development of IoT devices \cite{•}, there is a growing number of continuous infinite streams, \eg generated from sensors, which can be make use of. We can imagine researchers using continuous infinite streams as sources in workflows to make, for example, preliminary transformations to raw data in the future. However, because many systems are task-oriented, they are locked down to finite data so are not able to support this fashion. Therefore, we can say that data-streaming is the future. Thus, we expect to give mor e dynamics to data-streaming WMSs to better satisfy not only today's but also tomorrow's needs of researchers.

\section{Dispel4py}
\Dpy is a data-streaming pipeline-fashion WMS. It it built on top of the design of the Dispel workflow specification language \cite{atkinson2012data} while aligns more to scientists \cite{doi:10.1177/1094342016649766}. The most significant benefit of \dpy is that it maps the execution of workflows automatically to other platforms (\eg MPI) so it requires no user attention as long as the libraries needed are properly installed. This characteristic makes \dpy a comprehensive system, which is lacked by many systems. Moreover, because of the wide availability of Python, the large number of high-quality libraries for Python, it is easy to write new workflows or PEs (see next paragraph) for \dpy, and the ability to interoperate with C/C++ codes makes it possible to wrap existing workflows to execute in \dpy. These make it very easy to migrate to or use \dpy, and  PEs written in pure Python are usually platform-independent so sharing PEs and workflows is also very easy in \dpy.

In \dpy, the basic component is called Processing Element (PE). Generally, each PE corresponds to a task in the workflow. Each PE has zero or more inputs, and zero or more outputs (but not likely to be both zero because it's useless). A PE itself doesn't know where its inputs are from and where its outputs are to, which decouples PEs.

A workflow in \dpy is constructed by connecting output connections from one PE and input connections from another PE.

An example (split-merge) of workflow construction is made in \dpy is shown in Listing \ref{lst:wf_example} (taken from the original \dpy paper \cite{doi:10.1177/1094342016649766}).

\begin{lstlisting}[frame=single,caption={Example code of workflow construction in \dpy},captionpos=b,label={lst:wf_example},language=Python]
from dispel4py.workflow_graph import WorkflowGraph

pe1 = WordNumber()
pe2 = CountWord()
pe3 = Average()
pe4 = Reduce()

graph = WorkflowGraph()
graph.connect(pe1, 'output1', pe2, 'input')
graph.connect(pe1, 'output2', pe3, 'input')
graph.connect(pe2, 'output', pe4, 'input1')
graph.connect(pe3, 'output', pe4, 'input2')
\end{lstlisting}

\defTerm{PETmpl}{PE template}
\defTerm{PEInst}{PE}
\defTerm{PEDup}{PE instance}

There is a conceptual difference between \tPETmpl{}s, \tPEInst{}s and \tPEDup{}s, which may worth mentioning: a \tPETmpl is the logic (template) for processing data, usually implemented as a Python class; a \tPEInst is an instance (after construction) of such a class (\tPETmpl); \tPEDup is used only when we are talking about the (deliberate) duplications (copies) of the same \tPEInst. A \tPETmpl can be used to construct many different \tPEInst{}s and they can all behave differently because of, for example, the different parameters used to call the constructor. By default, each \tPEInst only has one \tPEDup so we usually don't distinguish between them (and we will favour the term \tPEInst); however, when deliberate duplication happens (\eg for better throughput as suggested by \citeauthor{doi:10.1177/1094342016649766} \cite{doi:10.1177/1094342016649766}), we will talk about \tPEDup{}s for better clarity.

The execution of \dpy is done by mapping the workflow to an execution platform (\ie an DCI), such as MPI, Storm or Multiprocessing which are currently supported by \dpy. This mapping behaviour is the benefit of the abstraction that \dpy brings because developers for workflows don't need to know anything about the platform (\eg hardware or middleware) that the workflow is going to be executed. This behaviour also gives the developers of \dpy freedom to optimize or extend the actual working procedures of \dpy without needing to worry about backward compatibilities too much.

For example, one of the current ``problem'' in \dpy is that it maps the whole workflow graph at once in the beginning, which may consume much time. As the method how workflows are constructed presents, the workflow in \dpy is a directed acyclic graph (DAG). Therefore, we can perform topological sorting to the graph, and obtain the sources by picking up the nodes with zero in-degrees. Thus, it is possible that we can deploy nodes only when needed (\eg when there are data sending to it) so we don't have to allocate all the resources in the beginning. This is the basic point of our first modification - \tincdep. By using this way, out modification works a bit similar to task-oriented WMSs which also use topological sorting for this purpose.

But we don't stop here - we also make use of the dynamics that \tincdep provides to achieve other goals. That is: because we can deploy PEs incrementally, we can also perform other modification to out workflow graph incrementally according to some rules. This is how our second target, \tdynexp, is done: to dynamically expand the workflow according to needs.

