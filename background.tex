\chapter{Background}
\sout{List of some workflow execution systems. Their features and differences to dispel4py. \\
What problem researchers are facing nowadays, and how they are related to our optimization (or dispel4py).}

As said in the Introduction chapter, workflows should be executed in a workflow management system (WMS). Different WMSs have different characteristics, so they can not be roughly unified.
 Generally, WMSs can be divided into two kinds by the way they schedule tasks and transport data:
\begin{enumerate}
	\item Task oriented
	\item Data streaming
\end{enumerate}

Task-oriented WMSs usually decompose the workflow into each task, and execute them separately in different phases / stages. The data produced by each task will usually be stored to disk and feed into the following task (and then may be deleted). Frameworks like Pegasus\cite{deelman2015pegasus}, Kepler\cite{ludascher2006scientific} are all developed as task-oriented workflow management systems. The benefit of task-oriented WMSs is they have the entire control of the workflow execution and can schedule the deployment according to needs (\ie the user can easily stop the execution at some point); they can also provide better fault-tolerance because data of each stage are cached on disk. However, this property is also its weakness: splitting tasks into stages will force a latter task to wait until all its previous tasks has finished, so the time needed will be longer. Moreover, it won't be able to support a continuous infinite data source because the stage used to execute that source will be infinite.

Data-streaming WMSs, on the other hand, streams data directly from the prior tasks to the following tasks in a pipeline fashion. Therefore, it will naturally support infinite data sources and there is no stages so no time will be wasted in waiting. As a result, the total execution time will be lower and users can get preliminary (partial) data when the first unit of outputs come out of the last task(s). On the contrary, fault tolerance of data-streaming WMSs will be weaker because there is no default mechanism caching intermediate data (so if a node fails, the whole workflow will need to restart). Even though, this weakness can be alleviated a bit by introducing intermediate tasks which passes the data while persisting them.

Although there are already several systems in the field, they don't completely satisfy the needs of current or near-future researchers \cite{•}. For example, with the development of IoT devices \cite{•}, there is a growing number of continuous infinite streams, \eg generated from sensors, which can be make use of. We can imagine researchers using continuous infinite streams as sources in workflows to make, for example, preliminary transformations to raw data in the future. However, because many systems are task-oriented, they are locked down to finite data so are not able to support this fashion. Therefore, we can say that data-streaming is the future. Thus, we expect to give mor e dynamics to data-streaming WMSs to better satisfy not only today's but also tomorrow's needs of researchers.

Dispel4py is a data-streaming pipeline-fashion WMS. In dispel4py, the basic component is called Processing Element (PE). Generally, each PE corresponds to a task in the workflow. Each PE has zero or more inputs, and zero or more outputs (but not likely to be both zero because useless); PEs are connected by matching inputs and outputs, and they forms the workflow. As the method how workflows are constructed presents, the workflow in dispel4py is a directed graph. Therefore, we can perform topological sorting to the graph, and obtain the sources - picking up the nodes with zero in-degrees. Therefore, it is possible that we can deploy nodes only when needed (\eg when there are data sending to it) so we don't have to allocate all the resources in the beginning. This is the basic point of our first modification - \emph{incremental deployment}. By using this way, out modification works a bit similar to task-oriented WMSs which also use topological sorting for this purpose. But we don't stop here - we also make use of the dynamics that incremental deployment provides to achieve other goals. That is: because we can deploy PEs incrementally, we can also perform other modification to out workflow graph incrementally according to some rules. This is how our second target, \emph{dynamic expansion}, is done: to dynamically expand the workflow according to needs.

